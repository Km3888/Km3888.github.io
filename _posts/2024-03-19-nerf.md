---
title: 'Neural Radiance Fields Explained'
date: 2023-06-15
permalink: /posts/2024/03/NeRF
tags:
  - Neural Radiance Fields
  - 3D Machine Learning
---


## 3D Scene Representation
Representing a 3D scene


![reconstruction](/images/nerf_images/novel_views.png)
generating 3D content, which can be applied toward virtual endeavors like animation, video games, VR/AR ([spatial computing](https://tinyurl.com/zky5tk3x) if you're cool) or brought into the world through 3D printing and AI-powered manufacturing. 

![dreamfusion](/images/nerf_images/dreamfusion.gif)

I'll mainly rely on 3D reconstruction as a motivating example since this post is about NeRFs and 3D reconstruction happens to be the canonical task for NeRFs that spurred their development. Regardless, it's not the specific application that matters here as much as the underlying idea which is how to find a fast, flexible, and efficient method for representing data that lives in 3 dimensions.

A commensense way of representing 3D content would be to discretize the content into a 3-dimensional grid in $\mathbb{R}^{ N \times N \times N \times 4}$. Here, each [voxel](https://en.wikipedia.org/wiki/Voxel) would store four values: The RGB color value for this region as well as the *opacity at this point*. Opacity is a value between $0$ and $1$ which essentially just indicates whether something solid (i.e can't be seen through) is located there. By storing just these four numbers in each location we are able to represent the scene's *radiance properties*, which determine how it reflects light and therefore how it will appear when viewed from any angle. 

The only loss of information here is going to be due to the discretization as we're using a fixed resolution $N$ in each dimension. This dissipates as we increase the resolution but doing so incurs significant memory overhead, as we have to store $O(N^3)$ voxel values each with their own RGB and opacity values. Using $N=64$ already requires storing a million values and scaling up to $N=128$ yields an eightfold increase which quickly gets out of hand. This is why using voxel grids often ends up being an all-around pain ([trust me, I know](https://nyu-dice-lab.github.io/ZeroForge/)). For reference, the picture below is a $128 \times 128$ rendition of a well-known painting. So if we want to be able to render images at higher resolution than this, our memory overhead is going to be enormous and in most cases larger than we can stomach. 

![](/images/nerf_images/128_image_resized.png)

This voxel grid format is just one simple example of what are called *explicit* methods for 3D representation. It's called this because it involves explicity storing values tied to their location, in this case using voxels. There are plenty of other explicit methods that tend to do a better job like [meshes](https://www.techtarget.com/whatis/definition/3D-mesh), [pointclouds](https://en.wikipedia.org/wiki/Point_cloud), etc. but all of them tend to require a significant memory overhead. None of this is meant to bash explicit shape representations, but rather to motivate the use of NeRFs which are an *implicit* approach that offers several benefits. One of these is their ability to compactly represent 3-dimensional scenes/shapes by finding and exploiting the struture lying in them.

## Implicit Shape Representation (NeRFs)

To build toward an understanding of NeRFs, we can shift to seeing a 3D representation as a specific mathematical object called a *field*. From [Wikipedia](https://en.wikipedia.org/wiki/Vector_field): 

>In vector calculus and physics, a **vector field** is an assignment of a vector to each point in a space, most commonly Euclidean space $\mathbb{R}^n$.

 Note that the voxel grid in the previous section is already sort of doing this as it assigns a four-dimensional radiance vector to specific points in space. However, this falls short of the definition of a field, which would require a radiance vector for *every* possible point in euclidean space. To truly capture this **radiance field** we would want to express it in the form of a function which maps $(x,y,z)$ coordinates to radiance vectors so that an assignment can be generated for any location.

This is exactly the idea of NeRFs, which aim to fit this $\mathbb{R^3} \rightarrow \mathbb{R}^4$ mapping using the universal approximation power of a neural network. With the right parameters $\Theta$, a neural network $f_\Theta$ can implicitly store the color and opacity for all possible locations which allows us to generate renderings at whatever resolution we desire while only bearing the memory overheard of storing $\Theta$. 

![](/images/nerf_images/nerf_network.png)

Since we're just doing a vector to vector mapping, the typical architecture used for a NeRF is a vanilla neural network, often referred to as a [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron#:~:text=A%20multilayer%20perceptron%20(MLP)%20is,that%20is%20not%20linearly%20separable.), or MLP. The standard architecture introduced in the NeRF paper uses 8 hidden layers with 1024 neurons in each layer. In some cases, the object at a location may appear differently depending on the direction it is viewed from. To account for this, NeRFs also condition the network on a specific viewing direction specified as angles. This adds another two inputs to the neural network, bringing the total to five.

Note that a NeRF and its parameters are *unique to a single object*. This is somewhat atypical in machine learning where we want to use a dataset to learn how to make predictions on new and unseen data. Here, you can think of an observed view of an object as a single datapoint and observe that by fitting our NeRF to the object using these views we're able to make predictions about new and unseen viewing directions. This entire approach works because shapes tend to have discernible patterns and neural networks are effective pattern recognition tools that can represent these patterns rather than the literal form they give rise to. For this reason a NeRF-based approach would be comletely useless if applied to data comprised of random noise.

Later on we'll discuss in depth the procedure for training NeRFs and actually finding the "right" parameters $\Theta$, but for now we can assume that we have these parameters on hand and get better acquainted with NeRFs.

## Positional Encoding (this is the hard part)

As mentioned earlier, the network for our NeRF is reliant upon the $xyz$ coordinate values to represent the radiance field. Its performance is therefore entirely depend on its ability to discriminate within this input feature space which makes it somewhat unsurprising that one of the key breakthrough in getting NeRFs to work was a feature engineering trick aimed at these spatial coordinates.

What the authors of the original NeRF paper found was that instead of representing a coordinate value as a single scalar, they could instead map the coordinate to a higher-dimensional vector. This mapping is chosen such that the vector can still uniquely identify the scalar input while also gaining additional properties that make for a more neural-network-friendly representation. The specifics of this can be a bit tricky to understand so I'll try to give you a close look at the mathematical definitions along with some intuition in case the math doesn't make sense. I'll also further discourage you from closing this tab by noting that positional encodings are an important building block in transformers (though for different reasons) so by wrapping your head around this you'll be knocking out two birds with one stone.

![](/images/nerf_images/positional_encoding.png)

The basic idea is that if you have a bunch of sine and cosine functions at different frequencies and you evaluate all of these functions at the same input value $p$ then by looking at the set of outputs you should be able to recover $p$. For instance, looking at the graph of different curves, we can see that any vertical slice we take is going to intersect the curves in wildly different locations due to the rapidly divergent behavior of each curve. Because of this, we can represent a location on the $x-axis$ of the graph using this sequence of sines: $sin(\frac{p}{2}),sin(p),sin(2p),sin(4p)$ and by doing so we'll get a unique identifier of that location. By extending this and adding cosines, we can define a function $\alpha: \mathbb{R}\rightarrow \mathbb{R}^{2L}$


$$\alpha(p) = [\cos(2^l \bf{p}),\sin(2^l\bf{p}) ]_{l=0}^{L=1}$$

Now that we've established that we're not losing any information by replacing $p$ with $\alpha(p)$ the obvious question arises: What could we possibly be *gaining* by doing this? If our positional encoding just represents the same information as a scalar, just in a more convoluted and annoying form, then are we not just taking a clear step backward? In fact, this encoding seems to runs contrary to the generally desirable goal of distilling and compressing information into a minimally complex form.

The answer here mainly boils down to the fact that it helps NeRFs capture *high-frequency* patterns in spatial data. Because $\alpha$ makes use of waves of varying frequencies, some (but not all) of its component functions change very rapidly with respect to their input. This is essential for our NeRF to capture the fine-grained detail of in certain objects. Of course, not all the patterns we wish to capture are of a high frequency and we also want it to be easy for our network to smoothly trace less steep contours. This formulation allows a best-of-both-worlds solution as it makes NeRFs sensitive to small changes in coordinate space while also permitting them to *not* be sensitive to small changes in coordinate space when the downstream MLP decides that's what's needed. How flexible!

Of course, just an MLP by itself is *capable* of representing both high and low frequency functions (let's not contradict the [UAT](https://en.wikipedia.org/wiki/Universal_approximation_theorem)) it's just that neural networks are kind of lazy and have a tendency to find optima which have low-frequency solutions. Using these spatial encodings adds a bias towards proper shape representation, leading to better performance with fewer parameters. Below you can see an ablation from the original NeRF paper which shows the result of fitting a NeRF to a ground-truth 3D model with the removal of view-dependence (those additional $\theta,\phi$ inputs) and positional embedding. Without positional embedding, the fine-grained details are reduced to a blurry approximation.

![](/images/nerf_images/nerf_ablation.png)

Nowadays, many applications use a slightly more sophisticated form of spatial embedding introduced in the [Mip-NeRF](https://jonbarron.info/mipnerf/) paper or leverage a clever data structure to store explicit embeddings like in NVIDIA's [Instant-NGP](https://github.com/NVlabs/instant-ngp). However, most solutions are closely related to the ideas presented here and and are meant to provide application-specific improvements.

## Rendering an Image with NeRFs

We have $r_{\Theta}$ and $c_{\Theta}$ and 

### Raytracing

For simplicity, let's avoid thinking about rendering an entire image and just focus on how to capture a single pixel. To do this, you can build a mental picture of what goes in your camera to capture a pixel's worth of color when you point it at an object and click the button. Put plainly, your camera emits a ray of light from the pixel location which retrieves the pixel value. This ray shoots forward in a straight line toward the object until it makes contact with its surface. The pixel color is then determined by the RGB value at this infinitesimal point of contact.

To imbue this intuition with some mathematical rigor, we can define our ray as a function $\bf{r}(t)$ that maps a time value to the 3D position the ray gets to at that time. When "shooting" a scene from a given angle, it's trivial to trace this ray's linear trip through space. Using our NeRF parameters also enables us compute the color $c_{\Theta}(\bf{r}(t))$ and opacity $\sigma_{\Theta}(\bf{r}(t))$ that our ray will encounter at any time value.

>Note: For a more detailed treatment of raytracing (also called raymarching), you can see [this interactive demonstration](https://blog.maximeheckel.com/posts/painting-with-math-a-gentle-study-of-raymarching/)

### Neural Rendering

These are pretty much all the components required for us to get to a formula to compute the actual pixel value for a ray that is shot through our NeRF scene. The only further caveat is that the above description in which the ray simply "makes contact" with the object assumes that opacity is a binary value so the ray travels freely before all at once hitting something solid. In actuality, our operating definition of opacity only restricted it between $0$ and $1$. This was intentional, as using a continuous value allows us to represent partially opaque objects and makes the mapping from scene to image differentiable. In this setting, instead of being stopped abrutly it's more like our ray gets gradually worn down as it encounters non-opaque regions. This lessens the effect of the colors it reaches later which are only partially visible.  With that in mind, we can define the pixel color $C(r,\Theta)$ by integrating over the ray's path as it  accumulates colors it encounters in high-opacity regions.

$$ C(r,\Theta) = \int_{t_0}^{\infty} T(\bf{r},t) \underbrace{\sigma_{\Theta}(\bf{r}(t))}_{opacity} \underbrace{c_{\Theta}(\bf{r}(t))}_{color}\it{dt}$$

$$ T(t) = \exp(-\int_{t_n}^{t}\sigma_{\Theta}(\bf{r}(s))\it{ds})$$

The second formula for $T(t)$ expresses how much of the ray is left at that time and hasn't already been blocked out. Once the value of $T(t)$ becomes very close to $0$ we can stop our raytracing as the remaining domain will have a negligible effect on the overall color. It is this stopping criterion which allows us to avoid querying irrelevant regions.

In practice, these integrals are approximated using a finite sum, which gives us some easily computable color $\hat{C}(r,\theta) \approx C(r,\theta)$. It's also standard to include some cutoff point for the upper bound of the integral as some rays never make contact with our scene, leaving the corresponding pixel to be black. If you understand this procedure for obtaining the color of a single pixel, then it's then pretty trivial to scale up to computing to an entire image as this procedure can be easily parallelized across a GPU.

>Note: In practice NeRFs employ a technique called hierarchical volume sampling to get better efficiency when breaking integral computations into finite sums. I've left this out since it's not essential to understanding NeRFs but the interesed reader can find a detailed description in the [NeRF paper](https://arxiv.org/abs/2003.08934)

## Training A NeRF

Up until now we've just assumed that $\Theta$ has been chosen so that it correctly represents the radiance field for the scene being portrayed. Now we move to a more realistic case in which we make no assumptions about the quality of $f_{\Theta}$ and take it upon ourselves to adjust the parameters in a sensible direction. To accomplish this we'll derive a loss function that is both tractrable (can be computed efficiently) and differentiable (has well-defined gradients).

Since we wish to impose some idea of "correctness", we require a ground-truth for comparison purposes. This supervision is almost always in image-space as this is the most common way to observe 3D data. Lucky for us, we know how to use our NeRF parameters to render our shape into image space, getting pixel values $\hat{C} (\bf{r},\Theta)$. 

![](/images/nerf_images/reconstruction.png)

We can use this to perform 3D reconstruction where we are given a set of images which depict a shape or scene and want to optimize our shape representation to align with these images. As long as we know the camera parameters used to take the photos, we can see each pixel of these images as a ground truth value $C_{gt}(r)$ that is associated with a specific ray. To compute a training loss, we randomly sample a batch of rays $\mathcal{R}$ from our image set and compare it to our ray values we get from our NeRF.

$$\mathcal{L}_{recon} = \sum_{r\in \mathcal{R}}\|\hat{C}(\bf{r},\Theta) - C_{gt}(\bf{r}) \|_2^2 $$

Computing this is fairly straightforward and because our mapping from $\Theta$ to $\hat{C}(\bf{r},\Theta)$ is constructed to be fully differentiable, the loss can be backpropagated into our network parameters. Below you can see samples of how NeRFs evolve over the training procedure. I think this does a good job of illustrating why we need our opacity values to be continuous, even when we're only representing solid objects, as the true scenes emerge from clouds of partial opacity.

![NeRF optimization with InstantNGP](/images/nerf_images/instantngp-crop.gif)

To make this work in practice, the original NeRF methodology uses 4096 rays per batch and samples 128 points along each ray to get $\hat{C}(\bf{r},\Theta)$. Repeating this until convergence takes between 100k and 300k iterations.

### Text-Conditioned Training

As I said before, 3D reconstruction is the canonical setup for training NeRFs, but it's just one of many settings in which NeRFs can be used. To give you an additional example, I'll briefly touch on how this training can be extended to training a *text-conditioned* NeRF. Here, we have some textual description $t$ which specifies a shape we'd like for our NeRF to represent. 

We can actually define a fairly simple loss function for this goal and remarkably, this loss will still operate entirely in the image space. This is because we can take advantage of the shared image-text powers of OpenAI's [CLIP](https://openai.com/research/clip). Without delving too much into how CLIP is trained, it computes vector embeddings of *both images and text* so that these embeddings retain their semantic information. This means that for some image $\mathcal{I}$ which depicts a toaster and the text string "toaster" we can expect the inner product $\left<\text{CLIP}(I),\text{CLIP}("toaster")\right>$ to have a high value indicating their semantic similarity. 

Applying this to our NeRF, we can use CLIP similarity as a training objective in image space. Instead of rendering individual rays from $f_{\Theta}$, we can instead render an entire image $\mathcal{I}$ at once and then compute the images semantic similarity with our text prompt $t$. This naturally yields the following loss function:

$$\mathcal{L}_t = - \left<\mathcal{I}(\Theta),t \right>$$

This approach was first introduced in Google's [Dreamfields](https://ajayj.com/dreamfields) paper a few years ago and while further improvements have been proposed by others, the sheer simplicity of this still amazes me. I already included the 

## Wrapping Up


so impressive in fact that it's led some to question whether NeRFs as a whole are facing extinction

![dead](/images/nerf_images/dead.png)

I've mostly stuck to the NeRF formulation introduced in the original NeRF paper by Mildenhall et al. I've also included some notes about architectural changes introduced in later works where relevant but really most of the key building blocks were introduced in that work.

What's remarkable is that that Mildenhall paper was in 2020 meaning that NeRFs themselves are only 4 years old. 
